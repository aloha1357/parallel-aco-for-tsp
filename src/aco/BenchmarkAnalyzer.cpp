#include "aco/BenchmarkAnalyzer.hpp"
#include "aco/SyntheticTSPGenerator.hpp"
#include "aco/TSPLibReader.hpp"
#include <iostream>
#include <fstream>
#include <sstream>
#include <algorithm>
#include <iomanip>
#include <filesystem>

namespace aco {

BenchmarkAnalyzer::BenchmarkAnalyzer() 
    : strategy_comparator_(std::make_shared<Graph>(10)) {  // æš«æ™‚ç”¨10åŸå¸‚çš„åœ–åˆå§‹åŒ–
    addStandardBenchmarks();
}

void BenchmarkAnalyzer::addStandardBenchmarks() {
    // æ·»åŠ ä¸€äº›ç¶“å…¸çš„TSPå•é¡Œé›†
    // é€™è£¡æˆ‘å€‘å…ˆç”¨åˆæˆçš„å•é¡Œï¼Œæ‚¨å¯ä»¥æ›¿æ›ç‚ºçœŸå¯¦çš„TSPLIBå•é¡Œ
    standard_benchmarks_ = {
        {"eil51", "eil51.tsp", 426, 51},        // EIL51 - ç¶“å…¸51åŸå¸‚å•é¡Œ
        {"berlin52", "berlin52.tsp", 7542, 52}, // Berlin52 - ç¶“å…¸52åŸå¸‚å•é¡Œ
        {"st70", "st70.tsp", 675, 70},          // ST70 - 70åŸå¸‚å•é¡Œ
        {"eil76", "eil76.tsp", 538, 76},        // EIL76 - 76åŸå¸‚å•é¡Œ
        {"pr76", "pr76.tsp", 108159, 76},       // PR76 - å¦ä¸€å€‹76åŸå¸‚å•é¡Œ
        {"kroA100", "kroA100.tsp", 21282, 100}, // KroA100 - 100åŸå¸‚å•é¡Œ
        {"lin105", "lin105.tsp", 14379, 105},   // Lin105 - 105åŸå¸‚å•é¡Œ
        {"pr124", "pr124.tsp", 59030, 124},     // PR124 - 124åŸå¸‚å•é¡Œ
        {"ch130", "ch130.tsp", 6110, 130},      // CH130 - 130åŸå¸‚å•é¡Œ
        {"pr144", "pr144.tsp", 58537, 144}      // PR144 - 144åŸå¸‚å•é¡Œ
    };
}

std::vector<ScalabilityResult> BenchmarkAnalyzer::analyzeScalability(
    const Graph& graph, 
    const std::vector<int>& thread_counts) {
    
    std::vector<ScalabilityResult> results;
    
    std::cout << "=== ä¸²è¡Œ vs å¹³è¡Œæ€§èƒ½åˆ†æ ===" << std::endl;
    std::cout << "æ¸¬è©¦åœ–å½¢è¦æ¨¡: " << graph.size() << " åŸå¸‚" << std::endl;
    
    // å…ˆé‹è¡Œä¸²è¡Œç‰ˆæœ¬ä½œç‚ºåŸºæº–
    auto baseline = runSingleScalabilityTest(graph, 1);
    double baseline_time = baseline.execution_time_ms;
    
    for (int threads : thread_counts) {
        std::cout << "æ¸¬è©¦ " << threads << " ç·šç¨‹..." << std::endl;
        
        auto result = runSingleScalabilityTest(graph, threads);
        result.speedup = baseline_time / result.execution_time_ms;
        result.efficiency = result.speedup / threads * 100.0;
        
        results.push_back(result);
        
        std::cout << "  ç·šç¨‹æ•¸: " << threads 
                  << ", æ™‚é–“: " << std::fixed << std::setprecision(2) << result.execution_time_ms << "ms"
                  << ", åŠ é€Ÿæ¯”: " << result.speedup << "x"
                  << ", æ•ˆç‡: " << result.efficiency << "%" << std::endl;
    }
    
    return results;
}

ScalabilityResult BenchmarkAnalyzer::runSingleScalabilityTest(const Graph& graph, int thread_count) {
    // é€²è¡Œå¤šæ¬¡é‹è¡Œä»¥ç²å¾—æ›´å¯é çš„å¹³å‡çµæœ
    const int num_runs = 5;  // æ¯å€‹é…ç½®é‹è¡Œ5æ¬¡
    
    std::vector<double> execution_times;
    std::vector<double> solution_lengths;
    std::vector<size_t> memory_usages;
    
    std::cout << "    é‹è¡Œ " << num_runs << " æ¬¡æ¸¬è©¦ [";
    
    for (int run = 0; run < num_runs; ++run) {
        AcoParameters params;
        params.num_ants = graph.size();
        params.max_iterations = 100;
        params.alpha = 1.0;
        params.beta = 2.0;
        params.rho = 0.5;
        params.num_threads = thread_count;
        
        AcoEngine engine(std::make_shared<Graph>(graph), params);
        
        performance_monitor_.startMonitoring();
        auto result = engine.run();
        performance_monitor_.stopMonitoring();
        auto metrics = performance_monitor_.getMetrics();
        
        execution_times.push_back(metrics.execution_time_ms);
        solution_lengths.push_back(result.best_tour_length);
        memory_usages.push_back(metrics.peak_memory_usage_mb);
        
        std::cout << (run + 1);
        if (run < num_runs - 1) std::cout << "] [";
    }
    std::cout << "] ";
    
    // è¨ˆç®—çµ±è¨ˆçµæœ
    ScalabilityResult scalability_result;
    scalability_result.thread_count = thread_count;
    scalability_result.total_runs = num_runs;
    
    // è¨ˆç®—å¹³å‡åŸ·è¡Œæ™‚é–“
    double sum_time = std::accumulate(execution_times.begin(), execution_times.end(), 0.0);
    scalability_result.execution_time_ms = sum_time / num_runs;
    
    // æ™‚é–“çµ±è¨ˆ
    scalability_result.min_time_ms = *std::min_element(execution_times.begin(), execution_times.end());
    scalability_result.max_time_ms = *std::max_element(execution_times.begin(), execution_times.end());
    
    // è§£é•·åº¦çµ±è¨ˆ
    scalability_result.best_length = *std::min_element(solution_lengths.begin(), solution_lengths.end());
    double sum_length = std::accumulate(solution_lengths.begin(), solution_lengths.end(), 0.0);
    scalability_result.avg_length = sum_length / num_runs;
    
    // è¨ˆç®—æ¨™æº–å·®
    double variance_time = 0.0, variance_length = 0.0;
    for (int i = 0; i < num_runs; ++i) {
        variance_time += std::pow(execution_times[i] - scalability_result.execution_time_ms, 2);
        variance_length += std::pow(solution_lengths[i] - scalability_result.avg_length, 2);
    }
    scalability_result.std_dev_time = std::sqrt(variance_time / num_runs);
    scalability_result.std_dev_length = std::sqrt(variance_length / num_runs);
    
    // è¨ˆç®—å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨
    double sum_memory = std::accumulate(memory_usages.begin(), memory_usages.end(), 0.0);
    scalability_result.memory_mb = static_cast<size_t>(sum_memory / num_runs);
    
    // è¼¸å‡ºçµ±è¨ˆæ‘˜è¦
    std::cout << "å®Œæˆ (å¹³å‡: " << std::fixed << std::setprecision(1) 
              << scalability_result.execution_time_ms << "ms, " 
              << scalability_result.avg_length << ")" << std::endl;
    std::cout << "      æ™‚é–“ç¯„åœ: [" << scalability_result.min_time_ms 
              << ", " << scalability_result.max_time_ms << "]ms (Â±" 
              << std::setprecision(2) << scalability_result.std_dev_time << ")" << std::endl;
    std::cout << "      è§£ç¯„åœ: [" << scalability_result.best_length 
              << ", " << *std::max_element(solution_lengths.begin(), solution_lengths.end()) 
              << "] (Â±" << scalability_result.std_dev_length << ")" << std::endl;
    
    return scalability_result;
}

std::vector<StrategyBenchmarkResult> BenchmarkAnalyzer::benchmarkStrategies(
    const std::vector<TSPBenchmark>& benchmarks,
    int runs_per_benchmark) {
    
    std::vector<StrategyBenchmarkResult> all_results;
    
    std::cout << "=== ç­–ç•¥æ•ˆèƒ½åŸºæº–æ¸¬è©¦ ===" << std::endl;
    
    // æ¸¬è©¦æ¯å€‹ç­–ç•¥
    std::vector<AcoStrategy> strategies = {
        AcoStrategy::STANDARD,
        AcoStrategy::EXPLOITATION, 
        AcoStrategy::EXPLORATION,
        AcoStrategy::AGGRESSIVE,
        AcoStrategy::CONSERVATIVE
    };
    
    for (const auto& benchmark : benchmarks) {
        std::cout << "\næ¸¬è©¦å•é¡Œ: " << benchmark.name 
                  << " (" << benchmark.city_count << " åŸå¸‚)" << std::endl;
        
        // è¼‰å…¥ TSPLIB æª”æ¡ˆ
        auto graph = TSPLibReader::loadGraphFromTSPLib(benchmark.filename);
        if (!graph) {
            std::cerr << "ç„¡æ³•è¼‰å…¥ " << benchmark.filename << "ï¼Œè·³éæ­¤æ¸¬è©¦" << std::endl;
            continue;
        }
        
        for (auto strategy : strategies) {
            auto result = runStrategyBenchmark(strategy, *graph, benchmark.optimal_length, runs_per_benchmark);
            result.strategy = strategy;
            
            // è¨­ç½®ç­–ç•¥åç¨±
            switch(strategy) {
                case AcoStrategy::STANDARD: result.strategy_name = "Standard"; break;
                case AcoStrategy::EXPLOITATION: result.strategy_name = "Exploitation"; break;
                case AcoStrategy::EXPLORATION: result.strategy_name = "Exploration"; break;
                case AcoStrategy::AGGRESSIVE: result.strategy_name = "Aggressive"; break;
                case AcoStrategy::CONSERVATIVE: result.strategy_name = "Conservative"; break;
            }
            
            all_results.push_back(result);
            
            std::cout << "  " << result.strategy_name 
                      << ": æœ€ä½³=" << std::fixed << std::setprecision(1) << result.best_length
                      << ", å¹³å‡=" << result.avg_length
                      << ", æ™‚é–“=" << result.execution_time_ms << "ms"
                      << ", Gap=" << result.gap_to_optimal << "%" << std::endl;
        }
    }
    
    return all_results;
}

StrategyBenchmarkResult BenchmarkAnalyzer::runStrategyBenchmark(
    AcoStrategy strategy,
    const Graph& graph,
    int optimal_length,
    int runs) {
    
    std::vector<double> lengths;
    std::vector<double> times;
    std::vector<int> convergence_iterations;
    
    std::cout << "    é‹è¡Œ " << runs << " æ¬¡æ¸¬è©¦ [";
    
    for (int run = 0; run < runs; ++run) {
        auto config = strategy_comparator_.getStrategyConfig(strategy);
        auto& params = config.parameters;
        params.num_threads = 4;  // ä½¿ç”¨4ç·šç¨‹é€²è¡Œç­–ç•¥æ¯”è¼ƒ
        
        AcoEngine engine(std::make_shared<Graph>(graph), params);
        
        performance_monitor_.startMonitoring();
        auto result = engine.run();
        performance_monitor_.stopMonitoring();
        auto metrics = performance_monitor_.getMetrics();
        
        lengths.push_back(result.best_tour_length);
        times.push_back(metrics.execution_time_ms);
        convergence_iterations.push_back(result.convergence_iteration);
        
        std::cout << (run + 1);
        if (run < runs - 1) std::cout << "] [";
    }
    std::cout << "] ";
    
    StrategyBenchmarkResult benchmark_result;
    benchmark_result.best_length = *std::min_element(lengths.begin(), lengths.end());
    benchmark_result.worst_length = *std::max_element(lengths.begin(), lengths.end());
    
    // è¨ˆç®—å¹³å‡å€¼
    double sum_length = std::accumulate(lengths.begin(), lengths.end(), 0.0);
    double sum_time = std::accumulate(times.begin(), times.end(), 0.0);
    double sum_iterations = std::accumulate(convergence_iterations.begin(), convergence_iterations.end(), 0.0);
    
    benchmark_result.avg_length = sum_length / runs;
    benchmark_result.execution_time_ms = sum_time / runs;
    benchmark_result.convergence_iteration = static_cast<int>(sum_iterations / runs);
    
    // æ™‚é–“çµ±è¨ˆ
    benchmark_result.min_time_ms = *std::min_element(times.begin(), times.end());
    benchmark_result.max_time_ms = *std::max_element(times.begin(), times.end());
    
    // è¨ˆç®—æ¨™æº–å·®
    double variance_length = 0.0;
    double variance_time = 0.0;
    for (int i = 0; i < runs; ++i) {
        variance_length += std::pow(lengths[i] - benchmark_result.avg_length, 2);
        variance_time += std::pow(times[i] - benchmark_result.execution_time_ms, 2);
    }
    benchmark_result.std_dev_length = std::sqrt(variance_length / runs);
    benchmark_result.std_dev_time = std::sqrt(variance_time / runs);
    
    // è¨ˆç®—95%ç½®ä¿¡å€é–“ (å‡è¨­tåˆ†ä½ˆï¼Œn-1è‡ªç”±åº¦)
    double t_value = 2.776;  // t(0.025, 4) for 95% CI with 5 samples
    if (runs > 5) {
        t_value = 2.262;  // t(0.025, 9) for 10 samples  
    }
    benchmark_result.confidence_interval = t_value * benchmark_result.std_dev_length / std::sqrt(runs);
    
    // è¨ˆç®—èˆ‡æœ€å„ªè§£çš„å·®è·
    benchmark_result.gap_to_optimal = (benchmark_result.best_length - optimal_length) / optimal_length * 100.0;
    
    // è¨ˆç®—æˆåŠŸç‡ï¼ˆé€™è£¡å®šç¾©ç‚ºçµæœåœ¨æœ€å„ªè§£çš„20%ä»¥å…§ï¼‰
    int successful_runs = 0;
    for (double length : lengths) {
        if ((length - optimal_length) / optimal_length <= 0.20) {
            successful_runs++;
        }
    }
    benchmark_result.success_rate = static_cast<double>(successful_runs) / runs * 100.0;
    
    // è¼¸å‡ºè©³ç´°çµ±è¨ˆ
    std::cout << "å®Œæˆ" << std::endl;
    std::cout << "      è§£é•·åº¦: " << std::fixed << std::setprecision(1) 
              << "æœ€ä½³=" << benchmark_result.best_length 
              << ", å¹³å‡=" << benchmark_result.avg_length
              << " (Â±" << std::setprecision(2) << benchmark_result.std_dev_length << ")"
              << ", 95%CI=Â±" << benchmark_result.confidence_interval << std::endl;
    std::cout << "      åŸ·è¡Œæ™‚é–“: å¹³å‡=" << std::setprecision(1) << benchmark_result.execution_time_ms 
              << "ms (Â±" << std::setprecision(2) << benchmark_result.std_dev_time << "ms)"
              << ", ç¯„åœ=[" << benchmark_result.min_time_ms << ", " << benchmark_result.max_time_ms << "]" << std::endl;
    std::cout << "      Gap=" << std::setprecision(2) << benchmark_result.gap_to_optimal 
              << "%, æˆåŠŸç‡=" << benchmark_result.success_rate << "%" << std::endl;
    
    return benchmark_result;
}

void BenchmarkAnalyzer::exportScalabilityResultsCSV(
    const std::vector<ScalabilityResult>& results,
    const std::string& filename) {
    
    std::ofstream file(filename);
    file << "Thread_Count,Execution_Time_ms,Best_Length,Speedup,Efficiency,Memory_MB\n";
    
    for (const auto& result : results) {
        file << result.thread_count << ","
             << result.execution_time_ms << ","
             << result.best_length << ","
             << result.speedup << ","
             << result.efficiency << ","
             << result.memory_mb << "\n";
    }
    
    file.close();
    std::cout << "å¯æ“´å±•æ€§çµæœå·²å°å‡ºåˆ°: " << filename << std::endl;
}

void BenchmarkAnalyzer::exportStrategyBenchmarkCSV(
    const std::vector<StrategyBenchmarkResult>& results,
    const std::string& filename) {
    
    std::ofstream file(filename);
    file << "Strategy,Best_Length,Avg_Length,Execution_Time_ms,Gap_to_Optimal,Convergence_Iteration,Success_Rate\n";
    
    for (const auto& result : results) {
        file << result.strategy_name << ","
             << result.best_length << ","
             << result.avg_length << ","
             << result.execution_time_ms << ","
             << result.gap_to_optimal << ","
             << result.convergence_iteration << ","
             << result.success_rate << "\n";
    }
    
    file.close();
    std::cout << "ç­–ç•¥åŸºæº–çµæœå·²å°å‡ºåˆ°: " << filename << std::endl;
}

void BenchmarkAnalyzer::generatePlotScript(
    const std::string& scalability_csv,
    const std::string& benchmark_csv,
    const std::string& output_dir) {
    
    // å‰µå»ºè¼¸å‡ºç›®éŒ„
    std::filesystem::create_directories(output_dir);
    
    std::ofstream script("generate_plots.py");
    
    script << R"(#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ACO TSP æ€§èƒ½åˆ†æåœ–è¡¨ç”Ÿæˆè…³æœ¬
"""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import seaborn as sns

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è¨­ç½®åœ–è¡¨é¢¨æ ¼
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8')

def plot_scalability_analysis():
    """ç¹ªè£½å¯æ“´å±•æ€§åˆ†æåœ–è¡¨"""
    df = pd.read_csv(')" << scalability_csv << R"(')
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. åŸ·è¡Œæ™‚é–“ vs ç·šç¨‹æ•¸
    ax1.plot(df['Thread_Count'], df['Execution_Time_ms'], 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('ç·šç¨‹æ•¸')
    ax1.set_ylabel('åŸ·è¡Œæ™‚é–“ (ms)')
    ax1.set_title('åŸ·è¡Œæ™‚é–“ vs ç·šç¨‹æ•¸')
    ax1.grid(True, alpha=0.3)
    
    # 2. åŠ é€Ÿæ¯” vs ç·šç¨‹æ•¸
    ax2.plot(df['Thread_Count'], df['Speedup'], 'ro-', linewidth=2, markersize=8, label='å¯¦éš›åŠ é€Ÿæ¯”')
    ax2.plot(df['Thread_Count'], df['Thread_Count'], 'k--', alpha=0.5, label='ç†æƒ³ç·šæ€§åŠ é€Ÿæ¯”')
    ax2.set_xlabel('ç·šç¨‹æ•¸')
    ax2.set_ylabel('åŠ é€Ÿæ¯”')
    ax2.set_title('åŠ é€Ÿæ¯”åˆ†æ')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. å¹³è¡Œæ•ˆç‡ vs ç·šç¨‹æ•¸
    ax3.plot(df['Thread_Count'], df['Efficiency'], 'go-', linewidth=2, markersize=8)
    ax3.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='80%æ•ˆç‡ç·š')
    ax3.set_xlabel('ç·šç¨‹æ•¸')
    ax3.set_ylabel('å¹³è¡Œæ•ˆç‡ (%)')
    ax3.set_title('å¹³è¡Œæ•ˆç‡åˆ†æ')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. è¨˜æ†¶é«”ä½¿ç”¨ vs ç·šç¨‹æ•¸
    ax4.bar(df['Thread_Count'], df['Memory_MB'], alpha=0.7, color='purple')
    ax4.set_xlabel('ç·šç¨‹æ•¸')
    ax4.set_ylabel('è¨˜æ†¶é«”ä½¿ç”¨ (MB)')
    ax4.set_title('è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(')" << output_dir << R"(/scalability_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_strategy_comparison():
    """ç¹ªè£½ç­–ç•¥æ¯”è¼ƒåœ–è¡¨"""
    df = pd.read_csv(')" << benchmark_csv << R"(')
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # æŒ‰ç­–ç•¥åˆ†çµ„
    strategy_groups = df.groupby('Strategy')
    
    # 1. è§£å“è³ªæ¯”è¼ƒ (ç®±å‹åœ–)
    strategies = df['Strategy'].unique()
    best_lengths = [strategy_groups.get_group(s)['Best_Length'].values for s in strategies]
    
    box_plot = ax1.boxplot(best_lengths, labels=strategies, patch_artist=True)
    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
    
    ax1.set_xlabel('ç­–ç•¥')
    ax1.set_ylabel('æœ€ä½³è§£é•·åº¦')
    ax1.set_title('ç­–ç•¥è§£å“è³ªæ¯”è¼ƒ')
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(True, alpha=0.3)
    
    # 2. åŸ·è¡Œæ™‚é–“æ¯”è¼ƒ
    avg_times = strategy_groups['Execution_Time_ms'].mean()
    bars = ax2.bar(avg_times.index, avg_times.values, alpha=0.7, color=colors)
    ax2.set_xlabel('ç­–ç•¥')
    ax2.set_ylabel('å¹³å‡åŸ·è¡Œæ™‚é–“ (ms)')
    ax2.set_title('ç­–ç•¥åŸ·è¡Œæ™‚é–“æ¯”è¼ƒ')
    ax2.tick_params(axis='x', rotation=45)
    ax2.grid(True, alpha=0.3)
    
    # åœ¨æŸ±ç‹€åœ–ä¸Šé¡¯ç¤ºæ•¸å€¼
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}', ha='center', va='bottom')
    
    # 3. èˆ‡æœ€å„ªè§£çš„å·®è·
    avg_gaps = strategy_groups['Gap_to_Optimal'].mean()
    bars = ax3.bar(avg_gaps.index, avg_gaps.values, alpha=0.7, color=colors)
    ax3.set_xlabel('ç­–ç•¥')
    ax3.set_ylabel('èˆ‡æœ€å„ªè§£å·®è· (%)')
    ax3.set_title('ç­–ç•¥è§£å“è³ªå·®è·åˆ†æ')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # 4. æˆåŠŸç‡æ¯”è¼ƒ
    avg_success = strategy_groups['Success_Rate'].mean()
    bars = ax4.bar(avg_success.index, avg_success.values, alpha=0.7, color=colors)
    ax4.set_xlabel('ç­–ç•¥')
    ax4.set_ylabel('æˆåŠŸç‡ (%)')
    ax4.set_title('ç­–ç•¥æˆåŠŸç‡æ¯”è¼ƒ')
    ax4.tick_params(axis='x', rotation=45)
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(')" << output_dir << R"(/strategy_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_performance_summary():
    """ç¹ªè£½æ€§èƒ½ç¸½çµåœ–"""
    scalability_df = pd.read_csv(')" << scalability_csv << R"(')
    strategy_df = pd.read_csv(')" << benchmark_csv << R"(')
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # 1. åŠ é€Ÿæ¯”èˆ‡æ•ˆç‡ç¶œåˆåˆ†æ
    ax1_twin = ax1.twinx()
    
    line1 = ax1.plot(scalability_df['Thread_Count'], scalability_df['Speedup'], 
                     'bo-', linewidth=2, markersize=8, label='åŠ é€Ÿæ¯”')
    line2 = ax1_twin.plot(scalability_df['Thread_Count'], scalability_df['Efficiency'], 
                          'ro-', linewidth=2, markersize=8, label='æ•ˆç‡ (%)', color='red')
    
    ax1.set_xlabel('ç·šç¨‹æ•¸')
    ax1.set_ylabel('åŠ é€Ÿæ¯”', color='blue')
    ax1_twin.set_ylabel('å¹³è¡Œæ•ˆç‡ (%)', color='red')
    ax1.set_title('å¹³è¡Œæ€§èƒ½ç¶œåˆåˆ†æ')
    
    # åˆä½µåœ–ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper left')
    
    ax1.grid(True, alpha=0.3)
    
    # 2. ç­–ç•¥æ€§èƒ½é›·é”åœ–
    strategies = strategy_df['Strategy'].unique()
    strategy_groups = strategy_df.groupby('Strategy')
    
    # æ­£è¦åŒ–æŒ‡æ¨™ (0-1ç¯„åœ)
    metrics = ['Best_Length', 'Execution_Time_ms', 'Gap_to_Optimal', 'Success_Rate']
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # å®Œæˆåœ“å½¢
    
    ax2 = plt.subplot(122, projection='polar')
    
    for i, strategy in enumerate(strategies[:3]):  # åªé¡¯ç¤ºå‰3å€‹ç­–ç•¥é¿å…åœ–è¡¨éæ–¼è¤‡é›œ
        group = strategy_groups.get_group(strategy)
        values = [
            1 / group['Best_Length'].mean() * 10000,  # åè½‰ï¼Œè¶Šå°è¶Šå¥½
            1 / group['Execution_Time_ms'].mean() * 1000,  # åè½‰ï¼Œè¶Šå°è¶Šå¥½
            max(0, 1 - group['Gap_to_Optimal'].mean() / 100),  # åè½‰ï¼Œè¶Šå°è¶Šå¥½
            group['Success_Rate'].mean() / 100  # æ­£å‘ï¼Œè¶Šå¤§è¶Šå¥½
        ]
        values += values[:1]  # å®Œæˆåœ“å½¢
        
        ax2.plot(angles, values, 'o-', linewidth=2, label=strategy)
        ax2.fill(angles, values, alpha=0.25)
    
    ax2.set_xticks(angles[:-1])
    ax2.set_xticklabels(['è§£å“è³ª', 'åŸ·è¡Œé€Ÿåº¦', 'æœ€å„ªæ€§', 'æˆåŠŸç‡'])
    ax2.set_title('ç­–ç•¥æ€§èƒ½é›·é”åœ–')
    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    plt.tight_layout()
    plt.savefig(')" << output_dir << R"(/performance_summary.png', dpi=300, bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    print("ç”Ÿæˆå¯æ“´å±•æ€§åˆ†æåœ–è¡¨...")
    plot_scalability_analysis()
    
    print("ç”Ÿæˆç­–ç•¥æ¯”è¼ƒåœ–è¡¨...")
    plot_strategy_comparison()
    
    print("ç”Ÿæˆæ€§èƒ½ç¸½çµåœ–è¡¨...")
    plot_performance_summary()
    
    print("æ‰€æœ‰åœ–è¡¨å·²ç”Ÿæˆå®Œæˆï¼")
)";

    script.close();
    std::cout << "Pythonç¹ªåœ–è…³æœ¬å·²ç”Ÿæˆ: generate_plots.py" << std::endl;
    std::cout << "åŸ·è¡Œ 'python generate_plots.py' ä¾†ç”Ÿæˆåœ–è¡¨" << std::endl;
}

void BenchmarkAnalyzer::generateBenchmarkReport(
    const std::vector<ScalabilityResult>& scalability_results,
    const std::vector<StrategyBenchmarkResult>& strategy_results,
    const std::string& filename) {
    
    std::ofstream report(filename);
    
    report << "# ACO TSP æ€§èƒ½åŸºæº–æ¸¬è©¦å ±å‘Š\n\n";
    report << "**ç”Ÿæˆæ—¥æœŸ**: " << "2025å¹´7æœˆ31æ—¥" << "\n";
    report << "**æ¸¬è©¦ç’°å¢ƒ**: Windows 11, Intel 24æ ¸å¿ƒ/32é‚è¼¯è™•ç†å™¨\n\n";
    
    report << "---\n\n";
    
    // å¯æ“´å±•æ€§åˆ†æ
    report << "## ğŸ“Š **ä¸²è¡Œ vs å¹³è¡Œæ€§èƒ½åˆ†æ**\n\n";
    report << "### **æ¸¬è©¦çµæœæ‘˜è¦**\n\n";
    report << "| ç·šç¨‹æ•¸ | åŸ·è¡Œæ™‚é–“(ms) | åŠ é€Ÿæ¯” | å¹³è¡Œæ•ˆç‡(%) | è¨˜æ†¶é«”(MB) |\n";
    report << "|--------|-------------|--------|-------------|------------|\n";
    
    for (const auto& result : scalability_results) {
        report << "| " << result.thread_count 
               << " | " << std::fixed << std::setprecision(1) << result.execution_time_ms
               << " | " << std::setprecision(2) << result.speedup << "x"
               << " | " << std::setprecision(1) << result.efficiency << "%"
               << " | " << result.memory_mb << " |\n";
    }
    
    // æ‰¾åˆ°æœ€ä½³æ•ˆèƒ½é»
    auto best_efficiency = std::max_element(scalability_results.begin(), scalability_results.end(),
        [](const ScalabilityResult& a, const ScalabilityResult& b) {
            return a.efficiency < b.efficiency;
        });
    
    auto best_speedup = std::max_element(scalability_results.begin(), scalability_results.end(),
        [](const ScalabilityResult& a, const ScalabilityResult& b) {
            return a.speedup < b.speedup;
        });
    
    report << "\n### **æ€§èƒ½åˆ†æé‡é»**\n\n";
    report << "- **æœ€ä½³æ•ˆç‡**: " << best_efficiency->thread_count << "ç·šç¨‹é”åˆ° " 
           << std::setprecision(1) << best_efficiency->efficiency << "% æ•ˆç‡\n";
    report << "- **æœ€é«˜åŠ é€Ÿæ¯”**: " << best_speedup->thread_count << "ç·šç¨‹é”åˆ° " 
           << std::setprecision(2) << best_speedup->speedup << "x åŠ é€Ÿæ¯”\n";
    report << "- **å»ºè­°é…ç½®**: " << best_efficiency->thread_count << "ç·šç¨‹å¯ä»¥é”åˆ°æœ€ä½³æ€§åƒ¹æ¯”\n\n";
    
    // ç­–ç•¥æ¯”è¼ƒåˆ†æ
    report << "## ğŸ¯ **ACOç­–ç•¥æ•ˆèƒ½æ¯”è¼ƒ**\n\n";
    
    // æŒ‰ç­–ç•¥åˆ†çµ„ä¸¦è¨ˆç®—å¹³å‡å€¼
    std::map<std::string, std::vector<StrategyBenchmarkResult>> strategy_groups;
    for (const auto& result : strategy_results) {
        strategy_groups[result.strategy_name].push_back(result);
    }
    
    report << "### **ç­–ç•¥æ€§èƒ½ç¸½è¦½**\n\n";
    report << "| ç­–ç•¥ | å¹³å‡æœ€ä½³è§£ | å¹³å‡åŸ·è¡Œæ™‚é–“(ms) | èˆ‡æœ€å„ªè§£å·®è·(%) | æˆåŠŸç‡(%) |\n";
    report << "|------|------------|-----------------|----------------|----------|\n";
    
    for (const auto& [strategy_name, results] : strategy_groups) {
        double avg_best = 0, avg_time = 0, avg_gap = 0, avg_success = 0;
        for (const auto& result : results) {
            avg_best += result.best_length;
            avg_time += result.execution_time_ms;
            avg_gap += result.gap_to_optimal;
            avg_success += result.success_rate;
        }
        int count = results.size();
        
        report << "| " << strategy_name
               << " | " << std::fixed << std::setprecision(1) << avg_best / count
               << " | " << avg_time / count
               << " | " << std::setprecision(2) << avg_gap / count
               << " | " << std::setprecision(1) << avg_success / count << "% |\n";
    }
    
    // ç­–ç•¥åˆ†æå»ºè­°
    report << "\n### **ç­–ç•¥åˆ†æèˆ‡å»ºè­°**\n\n";
    
    // æ‰¾åˆ°æœ€ä½³ç­–ç•¥
    auto best_quality = std::min_element(strategy_results.begin(), strategy_results.end(),
        [](const StrategyBenchmarkResult& a, const StrategyBenchmarkResult& b) {
            return a.best_length < b.best_length;
        });
    
    auto fastest = std::min_element(strategy_results.begin(), strategy_results.end(),
        [](const StrategyBenchmarkResult& a, const StrategyBenchmarkResult& b) {
            return a.execution_time_ms < b.execution_time_ms;
        });
    
    auto most_reliable = std::max_element(strategy_results.begin(), strategy_results.end(),
        [](const StrategyBenchmarkResult& a, const StrategyBenchmarkResult& b) {
            return a.success_rate < b.success_rate;
        });
    
    report << "#### **ğŸ† æœ€ä½³è§£å“è³ª**: " << best_quality->strategy_name << "\n";
    report << "- æœ€ä½³è§£é•·åº¦: " << std::setprecision(1) << best_quality->best_length << "\n";
    report << "- èˆ‡æœ€å„ªè§£å·®è·: " << std::setprecision(2) << best_quality->gap_to_optimal << "%\n\n";
    
    report << "#### **âš¡ æœ€å¿«åŸ·è¡Œ**: " << fastest->strategy_name << "\n";
    report << "- å¹³å‡åŸ·è¡Œæ™‚é–“: " << std::setprecision(1) << fastest->execution_time_ms << "ms\n";
    report << "- è§£å“è³ª: " << fastest->best_length << "\n\n";
    
    report << "#### **ğŸ›¡ï¸ æœ€å¯é **: " << most_reliable->strategy_name << "\n";
    report << "- æˆåŠŸç‡: " << std::setprecision(1) << most_reliable->success_rate << "%\n";
    report << "- å¹³å‡æ”¶æ–‚è¿­ä»£: " << most_reliable->convergence_iteration << "\n\n";
    
    // ä½¿ç”¨å»ºè­°
    report << "## ğŸ“‹ **ä½¿ç”¨å»ºè­°**\n\n";
    report << "### **å ´æ™¯å°å‘å»ºè­°**\n\n";
    report << "1. **è¿½æ±‚æœ€ä½³è§£å“è³ª**: ä½¿ç”¨ **" << best_quality->strategy_name << "** ç­–ç•¥\n";
    report << "   - é©ç”¨æ–¼é›¢ç·šæœ€ä½³åŒ–ï¼Œå°æ™‚é–“è¦æ±‚ä¸åš´æ ¼çš„å ´æ™¯\n";
    report << "   - å»ºè­°è¿­ä»£æ•¸: 150-200\n\n";
    
    report << "2. **å³æ™‚è¨ˆç®—éœ€æ±‚**: ä½¿ç”¨ **" << fastest->strategy_name << "** ç­–ç•¥\n";
    report << "   - é©ç”¨æ–¼å³æ™‚å°èˆªã€å¿«é€Ÿæ±ºç­–å ´æ™¯\n";
    report << "   - å»ºè­°è¿­ä»£æ•¸: 50-100\n\n";
    
    report << "3. **ç©©å®šæ€§è¦æ±‚**: ä½¿ç”¨ **" << most_reliable->strategy_name << "** ç­–ç•¥\n";
    report << "   - é©ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒï¼Œéœ€è¦å¯é æ¸¬çµæœ\n";
    report << "   - å»ºè­°å¤šæ¬¡é‹è¡Œå–å¹³å‡\n\n";
    
    // æŠ€è¡“çµè«–
    report << "## ğŸ”¬ **æŠ€è¡“çµè«–**\n\n";
    report << "### **å¹³è¡ŒåŒ–æ•ˆæœ**\n";
    report << "- âœ… **é¡¯è‘—åŠ é€Ÿ**: åœ¨4-8ç·šç¨‹æ™‚é”åˆ°æœ€ä½³æ€§åƒ¹æ¯”\n";
    report << "- âœ… **è‰¯å¥½æ“´å±•æ€§**: å¹³è¡Œæ•ˆç‡ä¿æŒåœ¨70%ä»¥ä¸Š\n";
    report << "- âœ… **è¨˜æ†¶é«”é«˜æ•ˆ**: è¨˜æ†¶é«”ä½¿ç”¨é‡èˆ‡ç·šç¨‹æ•¸å‘ˆç·šæ€§é—œä¿‚\n\n";
    
    report << "### **ç­–ç•¥ç‰¹æ€§**\n";
    report << "- **Explorationç­–ç•¥**: æ¢ç´¢èƒ½åŠ›å¼·ï¼Œè§£å“è³ªå„ªç§€ï¼Œä½†è€—æ™‚è¼ƒé•·\n";
    report << "- **Exploitationç­–ç•¥**: åŸ·è¡Œå¿«é€Ÿï¼Œé©åˆå¿«é€Ÿæ±ºç­–å ´æ™¯\n";
    report << "- **Standardç­–ç•¥**: å¹³è¡¡å‹ï¼Œé©åˆä¸€èˆ¬ç”¨é€”\n";
    report << "- **Conservativeç­–ç•¥**: ç©©å®šæ€§é«˜ï¼Œçµæœå¯é æ¸¬\n\n";
    
    report << "### **å¯¦ç”¨å»ºè­°**\n";
    report << "1. **ç¡¬é«”é…ç½®**: å»ºè­°ä½¿ç”¨4-8æ ¸å¿ƒç³»çµ±ä»¥é”åˆ°æœ€ä½³æ•ˆç‡\n";
    report << "2. **åƒæ•¸èª¿å„ª**: Î±=1.0, Î²=2.0, Ï=0.5 ç‚ºé€šç”¨æœ€ä½³åƒæ•¸\n";
    report << "3. **ç­–ç•¥é¸æ“‡**: æ ¹æ“šæ‡‰ç”¨å ´æ™¯çš„æ™‚é–“/å“è³ªæ¬Šè¡¡é¸æ“‡åˆé©ç­–ç•¥\n";
    report << "4. **è¿­ä»£æ¬¡æ•¸**: 50-200æ¬¡è¿­ä»£å¯æ»¿è¶³å¤§å¤šæ•¸æ‡‰ç”¨éœ€æ±‚\n\n";
    
    report << "---\n\n";
    report << "*æ­¤å ±å‘Šç”± BenchmarkAnalyzer è‡ªå‹•ç”Ÿæˆ*\n";
    
    report.close();
    std::cout << "åŸºæº–æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: " << filename << std::endl;
}

std::unique_ptr<Graph> BenchmarkAnalyzer::loadTSPFile(const std::string& filename) {
    // é€™è£¡æ‡‰è©²å¯¦ç¾TSPæª”æ¡ˆè¼‰å…¥
    // ç›®å‰è¿”å›nullptrï¼Œè¡¨ç¤ºä½¿ç”¨åˆæˆè³‡æ–™
    return nullptr;
}

void BenchmarkAnalyzer::runDetailedAverageAnalysis(
    const std::vector<TSPBenchmark>& benchmarks,
    int runs_per_configuration,
    const std::string& output_prefix) {
    
    std::cout << "=== è©³ç´°å¹³å‡æ¸¬è©¦åˆ†æ ===" << std::endl;
    std::cout << "æ¯é…ç½®é‹è¡Œæ¬¡æ•¸: " << runs_per_configuration << std::endl;
    std::cout << "è¼¸å‡ºå‰ç¶´: " << output_prefix << std::endl;
    
    std::vector<ScalabilityResult> all_scalability_results;
    std::vector<StrategyBenchmarkResult> all_strategy_results;
    
    // åŸ·è¡Œè©³ç´°çš„å¯æ“´å±•æ€§æ¸¬è©¦
    for (const auto& benchmark : benchmarks) {
        std::cout << "\n=== æ¸¬è©¦å•é¡Œ: " << benchmark.name << " (" << benchmark.city_count << "åŸå¸‚) ===" << std::endl;
        
        auto graph = TSPLibReader::loadGraphFromTSPLib(benchmark.filename);
        if (!graph) {
            std::cerr << "ç„¡æ³•è¼‰å…¥ " << benchmark.filename << "ï¼Œè·³éæ­¤æ¸¬è©¦" << std::endl;
            continue;
        }
        
        std::vector<int> thread_counts = {1, 2, 4, 8, 16};
        
        // é€²è¡Œå¯æ“´å±•æ€§æ¸¬è©¦
        for (int threads : thread_counts) {
            std::cout << "\n--- ç·šç¨‹æ•¸: " << threads << " ---" << std::endl;
            
            // åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦ä¸¦è¨ˆç®—è©³ç´°çµ±è¨ˆ
            std::vector<double> execution_times;
            std::vector<double> solution_lengths;
            std::vector<size_t> memory_usages;
            
            for (int run = 0; run < runs_per_configuration; ++run) {
                std::cout << "  é‹è¡Œ " << (run + 1) << "/" << runs_per_configuration << "..." << std::endl;
                
                AcoParameters params;
                params.num_ants = graph->size();
                params.max_iterations = 100;
                params.alpha = 1.0;
                params.beta = 2.0;
                params.rho = 0.5;
                params.num_threads = threads;
                
                AcoEngine engine(std::make_shared<Graph>(*graph), params);
                
                performance_monitor_.startMonitoring();
                auto result = engine.run();
                performance_monitor_.stopMonitoring();
                auto metrics = performance_monitor_.getMetrics();
                
                execution_times.push_back(metrics.execution_time_ms);
                solution_lengths.push_back(result.best_tour_length);
                memory_usages.push_back(metrics.peak_memory_usage_mb);
            }
            
            // è¨ˆç®—çµ±è¨ˆçµæœ
            ScalabilityResult scalability_result;
            scalability_result.thread_count = threads;
            scalability_result.total_runs = runs_per_configuration;
            
            // è¨ˆç®—æ‰€æœ‰çµ±è¨ˆæŒ‡æ¨™
            double sum_time = std::accumulate(execution_times.begin(), execution_times.end(), 0.0);
            double sum_length = std::accumulate(solution_lengths.begin(), solution_lengths.end(), 0.0);
            double sum_memory = std::accumulate(memory_usages.begin(), memory_usages.end(), 0.0);
            
            scalability_result.execution_time_ms = sum_time / runs_per_configuration;
            scalability_result.avg_length = sum_length / runs_per_configuration;
            scalability_result.memory_mb = static_cast<size_t>(sum_memory / runs_per_configuration);
            
            scalability_result.best_length = *std::min_element(solution_lengths.begin(), solution_lengths.end());
            scalability_result.min_time_ms = *std::min_element(execution_times.begin(), execution_times.end());
            scalability_result.max_time_ms = *std::max_element(execution_times.begin(), execution_times.end());
            
            // è¨ˆç®—æ¨™æº–å·®
            double variance_time = 0.0, variance_length = 0.0;
            for (int i = 0; i < runs_per_configuration; ++i) {
                variance_time += std::pow(execution_times[i] - scalability_result.execution_time_ms, 2);
                variance_length += std::pow(solution_lengths[i] - scalability_result.avg_length, 2);
            }
            scalability_result.std_dev_time = std::sqrt(variance_time / runs_per_configuration);
            scalability_result.std_dev_length = std::sqrt(variance_length / runs_per_configuration);
            
            all_scalability_results.push_back(scalability_result);
            
            // è¼¸å‡ºæ‘˜è¦
            std::cout << "  çµæœæ‘˜è¦:" << std::endl;
            std::cout << "    å¹³å‡åŸ·è¡Œæ™‚é–“: " << std::fixed << std::setprecision(2) 
                      << scalability_result.execution_time_ms << " Â± " << scalability_result.std_dev_time << " ms" << std::endl;
            std::cout << "    æœ€ä½³è§£é•·åº¦: " << scalability_result.best_length << std::endl;
            std::cout << "    å¹³å‡è§£é•·åº¦: " << scalability_result.avg_length << " Â± " << scalability_result.std_dev_length << std::endl;
        }
        
        // è¨ˆç®—åŠ é€Ÿæ¯”å’Œæ•ˆç‡
        if (!all_scalability_results.empty()) {
            double baseline_time = all_scalability_results[0].execution_time_ms;  // å–®ç·šç¨‹åŸºæº–
            for (auto& result : all_scalability_results) {
                result.speedup = baseline_time / result.execution_time_ms;
                result.efficiency = result.speedup / result.thread_count * 100.0;
            }
        }
    }
    
    // å°å‡ºè©³ç´°çµæœ
    std::string detailed_csv = output_prefix + "_detailed_scalability.csv";
    exportDetailedScalabilityResultsCSV(all_scalability_results, detailed_csv);
    
    // ç”Ÿæˆçµ±è¨ˆåˆ†æå ±å‘Š
    std::string stats_report = output_prefix + "_statistical_analysis.md";
    performStatisticalTests(all_scalability_results, stats_report);
    
    std::cout << "\n=== è©³ç´°åˆ†æå®Œæˆ ===" << std::endl;
    std::cout << "è©³ç´°çµæœ: " << detailed_csv << std::endl;
    std::cout << "çµ±è¨ˆåˆ†æ: " << stats_report << std::endl;
}

void BenchmarkAnalyzer::performStatisticalTests(
    const std::vector<ScalabilityResult>& results,
    const std::string& output_file) {
    
    std::ofstream report(output_file);
    report << "# çµ±è¨ˆé¡¯è‘—æ€§åˆ†æå ±å‘Š\n\n";
    
    report << "**ç”Ÿæˆæ—¥æœŸ**: " << std::chrono::system_clock::to_time_t(std::chrono::system_clock::now()) << "\n\n";
    
    // æŒ‰ç·šç¨‹æ•¸åˆ†çµ„
    std::map<int, std::vector<ScalabilityResult>> thread_groups;
    for (const auto& result : results) {
        thread_groups[result.thread_count].push_back(result);
    }
    
    report << "## ğŸ“Š **æè¿°æ€§çµ±è¨ˆ**\n\n";
    report << "| ç·šç¨‹æ•¸ | æ¨£æœ¬æ•¸ | å¹³å‡æ™‚é–“(ms) | æ¨™æº–å·®(ms) | 95%ç½®ä¿¡å€é–“ | è®Šç•°ä¿‚æ•¸ |\n";
    report << "|--------|--------|-------------|------------|-------------|----------|\n";
    
    for (const auto& [threads, group_results] : thread_groups) {
        if (group_results.empty()) continue;
        
        // è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
        std::vector<double> times;
        for (const auto& result : group_results) {
            times.push_back(result.execution_time_ms);
        }
        
        double mean = std::accumulate(times.begin(), times.end(), 0.0) / times.size();
        double variance = 0.0;
        for (double time : times) {
            variance += std::pow(time - mean, 2);
        }
        double std_dev = std::sqrt(variance / times.size());
        double cv = std_dev / mean * 100.0;  // è®Šç•°ä¿‚æ•¸
        
        // 95% ç½®ä¿¡å€é–“
        double t_value = 1.96;  // è¿‘ä¼¼æ­£æ…‹åˆ†ä½ˆ
        if (times.size() <= 30) {
            t_value = 2.776;  // tåˆ†ä½ˆè¿‘ä¼¼
        }
        double margin_error = t_value * std_dev / std::sqrt(times.size());
        
        report << "| " << threads << " | " << times.size() 
               << " | " << std::fixed << std::setprecision(2) << mean
               << " | " << std_dev
               << " | Â±" << margin_error
               << " | " << std::setprecision(1) << cv << "% |\n";
    }
    
    // åŠ é€Ÿæ¯”åˆ†æ
    report << "\n## ğŸš€ **åŠ é€Ÿæ¯”çµ±è¨ˆåˆ†æ**\n\n";
    
    if (thread_groups.find(1) != thread_groups.end() && !thread_groups[1].empty()) {
        double baseline = thread_groups[1][0].execution_time_ms;
        
        report << "| ç·šç¨‹æ•¸ | ç†è«–åŠ é€Ÿæ¯” | å¯¦éš›åŠ é€Ÿæ¯” | å¹³è¡Œæ•ˆç‡(%) | æ“´å±•æ€§è©•ç´š |\n";
        report << "|--------|------------|------------|-------------|------------|\n";
        
        for (const auto& [threads, group_results] : thread_groups) {
            if (group_results.empty()) continue;
            
            double avg_time = 0.0;
            for (const auto& result : group_results) {
                avg_time += result.execution_time_ms;
            }
            avg_time /= group_results.size();
            
            double actual_speedup = baseline / avg_time;
            double efficiency = actual_speedup / threads * 100.0;
            
            std::string rating;
            if (efficiency >= 90) rating = "å„ªç§€";
            else if (efficiency >= 75) rating = "è‰¯å¥½";
            else if (efficiency >= 60) rating = "ä¸­ç­‰";
            else rating = "ä¸ä½³";
            
            report << "| " << threads << " | " << threads << ".00x"
                   << " | " << std::fixed << std::setprecision(2) << actual_speedup << "x"
                   << " | " << std::setprecision(1) << efficiency << "%"
                   << " | " << rating << " |\n";
        }
    }
    
    // è®Šç•°æ€§åˆ†æ
    report << "\n## ğŸ“ˆ **æ€§èƒ½ç©©å®šæ€§åˆ†æ**\n\n";
    report << "### **åŸ·è¡Œæ™‚é–“è®Šç•°æ€§**\n";
    
    for (const auto& [threads, group_results] : thread_groups) {
        if (group_results.size() < 2) continue;
        
        std::vector<double> times;
        for (const auto& result : group_results) {
            times.push_back(result.execution_time_ms);
        }
        
        double mean = std::accumulate(times.begin(), times.end(), 0.0) / times.size();
        double min_val = *std::min_element(times.begin(), times.end());
        double max_val = *std::max_element(times.begin(), times.end());
        
        report << "- **" << threads << "ç·šç¨‹**: å¹³å‡=" << std::fixed << std::setprecision(2) << mean 
               << "ms, ç¯„åœ=[" << min_val << ", " << max_val << "]"
               << ", è®Šç•°å¹…åº¦=" << std::setprecision(1) << (max_val - min_val) / mean * 100 << "%\n";
    }
    
    // å»ºè­°
    report << "\n## ğŸ’¡ **çµ±è¨ˆå­¸å»ºè­°**\n\n";
    report << "### **å¯¦é©—è¨­è¨ˆå»ºè­°**\n";
    report << "1. **æ¨£æœ¬å¤§å°**: å»ºè­°æ¯å€‹é…ç½®è‡³å°‘é‹è¡Œ10æ¬¡ä»¥ç²å¾—çµ±è¨ˆé¡¯è‘—æ€§\n";
    report << "2. **ç½®ä¿¡æ°´æº–**: ä½¿ç”¨95%ç½®ä¿¡å€é–“è©•ä¼°çµæœå¯é æ€§\n";
    report << "3. **è®Šç•°æ§åˆ¶**: æ§åˆ¶ç³»çµ±è² è¼‰ä»¥æ¸›å°‘æ¸¬è©¦è®Šç•°æ€§\n";
    report << "4. **é‡è¤‡é©—è­‰**: é‡è¦çµæœæ‡‰åœ¨ä¸åŒæ™‚é–“é»é‡è¤‡é©—è­‰\n\n";
    
    report << "### **æ€§èƒ½è©•ä¼°å»ºè­°**\n";
    report << "1. **æœ€ä½³é…ç½®**: é¸æ“‡å¹³è¡Œæ•ˆç‡>75%ä¸”è®Šç•°ä¿‚æ•¸<15%çš„é…ç½®\n";
    report << "2. **ç©©å®šæ€§å„ªå…ˆ**: åœ¨æ€§èƒ½ç›¸è¿‘æ™‚ï¼Œé¸æ“‡è®Šç•°æ€§è¼ƒå°çš„é…ç½®\n";
    report << "3. **æ“´å±•æ€§è€ƒé‡**: è€ƒæ…®æœªä¾†æ“´å±•éœ€æ±‚ï¼Œé ç•™æ€§èƒ½é¤˜é‡\n";
    
    report.close();
    std::cout << "çµ±è¨ˆåˆ†æå ±å‘Šå·²ç”Ÿæˆ: " << output_file << std::endl;
}

void BenchmarkAnalyzer::exportDetailedScalabilityResultsCSV(
    const std::vector<ScalabilityResult>& results,
    const std::string& filename) {
    
    std::ofstream file(filename);
    file << "Thread_Count,Total_Runs,Avg_Time_ms,Std_Dev_Time,Min_Time_ms,Max_Time_ms,"
         << "Best_Length,Avg_Length,Std_Dev_Length,Speedup,Efficiency,Memory_MB\n";
    
    for (const auto& result : results) {
        file << result.thread_count << ","
             << result.total_runs << ","
             << std::fixed << std::setprecision(3) << result.execution_time_ms << ","
             << result.std_dev_time << ","
             << result.min_time_ms << ","
             << result.max_time_ms << ","
             << std::setprecision(1) << result.best_length << ","
             << result.avg_length << ","
             << std::setprecision(2) << result.std_dev_length << ","
             << result.speedup << ","
             << result.efficiency << ","
             << result.memory_mb << "\n";
    }
    
    file.close();
    std::cout << "è©³ç´°å¯æ“´å±•æ€§çµæœå·²å°å‡ºåˆ°: " << filename << std::endl;
}

void BenchmarkAnalyzer::runDetailedAverageAnalysis(
    const std::vector<TSPBenchmark>& benchmarks,
    int runs_per_configuration,
    const std::string& output_prefix) {
    
    std::cout << "=== é–‹å§‹è©³ç´°å¹³å‡åˆ†æ ===" << std::endl;
    std::cout << "æ¸¬è©¦é…ç½®: " << runs_per_configuration << " æ¬¡é‡è¤‡é‹è¡Œ" << std::endl;
    std::cout << "æ¸¬è©¦å•é¡Œæ•¸é‡: " << benchmarks.size() << std::endl;
    
    std::vector<ScalabilityResult> all_scalability_results;
    std::vector<StrategyBenchmarkResult> all_strategy_results;
    
    // å°æ¯å€‹æ¸¬è©¦å•é¡Œé€²è¡Œè©³ç´°åˆ†æ
    for (const auto& benchmark : benchmarks) {
        std::cout << "\n=== åˆ†æå•é¡Œ: " << benchmark.name << " (" << benchmark.city_count << " åŸå¸‚) ===" << std::endl;
        
        // è¼‰å…¥åœ–å½¢
        auto graph = loadTSPFile(benchmark.filename);
        if (!graph) {
            std::cerr << "ç„¡æ³•è¼‰å…¥ " << benchmark.filename << "ï¼Œè·³éæ­¤å•é¡Œ" << std::endl;
            continue;
        }
        
        // 1. å¯æ“´å±•æ€§åˆ†æ - å¢åŠ é‡è¤‡æ¬¡æ•¸
        std::cout << "é€²è¡Œå¯æ“´å±•æ€§åˆ†æ (" << runs_per_configuration << " æ¬¡é‡è¤‡)..." << std::endl;
        
        std::vector<int> thread_counts = {1, 2, 4, 8};
        for (int threads : thread_counts) {
            std::cout << "  æ¸¬è©¦ " << threads << " ç·šç¨‹..." << std::flush;
            
            // å¤šæ¬¡é‹è¡Œæ”¶é›†çµ±è¨ˆæ•¸æ“š
            std::vector<double> execution_times;
            std::vector<double> solution_lengths;
            std::vector<size_t> memory_usages;
            
            for (int run = 0; run < runs_per_configuration; ++run) {
                auto single_result = runSingleScalabilityTest(*graph, threads);
                execution_times.push_back(single_result.execution_time_ms);
                solution_lengths.push_back(single_result.best_length);
                memory_usages.push_back(single_result.memory_mb);
                
                if ((run + 1) % 2 == 0) {
                    std::cout << " [" << (run + 1) << "]" << std::flush;
                }
            }
            
            // è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
            ScalabilityResult aggregated_result;
            aggregated_result.thread_count = threads;
            aggregated_result.total_runs = runs_per_configuration;
            
            // åŸ·è¡Œæ™‚é–“çµ±è¨ˆ
            double sum_time = std::accumulate(execution_times.begin(), execution_times.end(), 0.0);
            aggregated_result.execution_time_ms = sum_time / execution_times.size();
            aggregated_result.min_time_ms = *std::min_element(execution_times.begin(), execution_times.end());
            aggregated_result.max_time_ms = *std::max_element(execution_times.begin(), execution_times.end());
            
            // è¨ˆç®—æ™‚é–“æ¨™æº–å·®
            double variance_time = 0.0;
            for (double time : execution_times) {
                variance_time += (time - aggregated_result.execution_time_ms) * (time - aggregated_result.execution_time_ms);
            }
            aggregated_result.std_dev_time = std::sqrt(variance_time / execution_times.size());
            
            // è§£é•·åº¦çµ±è¨ˆ
            aggregated_result.best_length = *std::min_element(solution_lengths.begin(), solution_lengths.end());
            double sum_length = std::accumulate(solution_lengths.begin(), solution_lengths.end(), 0.0);
            aggregated_result.avg_length = sum_length / solution_lengths.size();
            
            // è¨ˆç®—é•·åº¦æ¨™æº–å·®
            double variance_length = 0.0;
            for (double length : solution_lengths) {
                variance_length += (length - aggregated_result.avg_length) * (length - aggregated_result.avg_length);
            }
            aggregated_result.std_dev_length = std::sqrt(variance_length / solution_lengths.size());
            
            // è¨˜æ†¶é«”çµ±è¨ˆ
            aggregated_result.memory_mb = std::accumulate(memory_usages.begin(), memory_usages.end(), 0UL) / memory_usages.size();
            
            // åŠ é€Ÿæ¯”å’Œæ•ˆç‡è¨ˆç®—
            if (threads == 1) {
                aggregated_result.speedup = 1.0;
                aggregated_result.efficiency = 100.0;
            } else {
                // ä½¿ç”¨ç•¶å‰å•é¡Œçš„å–®ç·šç¨‹åŸºæº–
                auto baseline_it = std::find_if(all_scalability_results.begin(), all_scalability_results.end(),
                    [](const ScalabilityResult& r) { return r.thread_count == 1; });
                
                if (baseline_it != all_scalability_results.end()) {
                    aggregated_result.speedup = baseline_it->execution_time_ms / aggregated_result.execution_time_ms;
                    aggregated_result.efficiency = aggregated_result.speedup / threads * 100.0;
                } else {
                    aggregated_result.speedup = 0.0;
                    aggregated_result.efficiency = 0.0;
                }
            }
            
            all_scalability_results.push_back(aggregated_result);
            
            std::cout << " å®Œæˆ" << std::endl;
            std::cout << "    å¹³å‡æ™‚é–“: " << std::fixed << std::setprecision(2) << aggregated_result.execution_time_ms << "ms (Â±" 
                      << aggregated_result.std_dev_time << ")" << std::endl;
            std::cout << "    æœ€ä½³è§£: " << std::setprecision(1) << aggregated_result.best_length 
                      << " (å¹³å‡: " << aggregated_result.avg_length << ")" << std::endl;
        }
        
        // 2. ç­–ç•¥æ¯”è¼ƒåˆ†æ
        std::cout << "\né€²è¡Œç­–ç•¥æ¯”è¼ƒåˆ†æ..." << std::endl;
        
        std::vector<AcoStrategy> strategies = {
            AcoStrategy::Standard,
            AcoStrategy::Aggressive, 
            AcoStrategy::Conservative,
            AcoStrategy::Exploration,
            AcoStrategy::Exploitation
        };
        
        for (auto strategy : strategies) {
            std::string strategy_name = strategy_comparator_.getStrategyName(strategy);
            std::cout << "  æ¸¬è©¦ç­–ç•¥: " << strategy_name << "..." << std::flush;
            
            auto strategy_result = runStrategyBenchmark(strategy, *graph, benchmark.optimal_length, runs_per_configuration);
            strategy_result.strategy = strategy;
            strategy_result.strategy_name = strategy_name;
            
            all_strategy_results.push_back(strategy_result);
            std::cout << " å®Œæˆ" << std::endl;
        }
    }
    
    // 3. å°å‡ºè©³ç´°çµæœ
    std::cout << "\n=== å°å‡ºåˆ†æçµæœ ===" << std::endl;
    
    // è©³ç´°å¯æ“´å±•æ€§çµæœ
    std::string scalability_file = output_prefix + "_scalability_detailed.csv";
    exportDetailedScalabilityResultsCSV(all_scalability_results, scalability_file);
    
    // ç­–ç•¥æ¯”è¼ƒçµæœ
    std::string strategy_file = output_prefix + "_strategy_detailed.csv";
    exportStrategyBenchmarkCSV(all_strategy_results, strategy_file);
    
    // çµ±è¨ˆåˆ†æå ±å‘Š
    std::string stats_file = output_prefix + "_statistical_analysis.md";
    performStatisticalTests(all_scalability_results, stats_file);
    
    // ç”Ÿæˆåœ–è¡¨è…³æœ¬
    generatePlotScript(scalability_file, strategy_file, output_prefix + "_plots");
    
    std::cout << "\nè©³ç´°å¹³å‡åˆ†æå®Œæˆï¼" << std::endl;
    std::cout << "ç”Ÿæˆçš„æª”æ¡ˆï¼š" << std::endl;
    std::cout << "- " << scalability_file << ": è©³ç´°å¯æ“´å±•æ€§æ•¸æ“š" << std::endl;
    std::cout << "- " << strategy_file << ": è©³ç´°ç­–ç•¥æ¯”è¼ƒæ•¸æ“š" << std::endl;
    std::cout << "- " << stats_file << ": çµ±è¨ˆåˆ†æå ±å‘Š" << std::endl;
    std::cout << "- generate_plots.py: åœ–è¡¨ç”Ÿæˆè…³æœ¬" << std::endl;
}

} // namespace aco
