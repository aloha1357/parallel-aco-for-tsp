#include "aco/BenchmarkAnalyzer.hpp"
#include "aco/SyntheticTSPGenerator.hpp"
#include <iostream>
#include <fstream>
#include <sstream>
#include <algorithm>
#include <iomanip>
#include <filesystem>

namespace aco {

BenchmarkAnalyzer::BenchmarkAnalyzer() 
    : strategy_comparator_(std::make_shared<Graph>(10)) {  // æš«æ™‚ç”¨10åŸå¸‚çš„åœ–åˆå§‹åŒ–
    addStandardBenchmarks();
}

void BenchmarkAnalyzer::addStandardBenchmarks() {
    // æ·»åŠ ä¸€äº›ç¶“å…¸çš„TSPå•é¡Œé›†
    // é€™è£¡æˆ‘å€‘å…ˆç”¨åˆæˆçš„å•é¡Œï¼Œæ‚¨å¯ä»¥æ›¿æ›ç‚ºçœŸå¯¦çš„TSPLIBå•é¡Œ
    standard_benchmarks_ = {
        {"eil51", "eil51.tsp", 426, 51},        // EIL51 - ç¶“å…¸51åŸå¸‚å•é¡Œ
        {"berlin52", "berlin52.tsp", 7542, 52}, // Berlin52 - ç¶“å…¸52åŸå¸‚å•é¡Œ
        {"st70", "st70.tsp", 675, 70},          // ST70 - 70åŸå¸‚å•é¡Œ
        {"eil76", "eil76.tsp", 538, 76},        // EIL76 - 76åŸå¸‚å•é¡Œ
        {"pr76", "pr76.tsp", 108159, 76},       // PR76 - å¦ä¸€å€‹76åŸå¸‚å•é¡Œ
        {"kroA100", "kroA100.tsp", 21282, 100}, // KroA100 - 100åŸå¸‚å•é¡Œ
        {"lin105", "lin105.tsp", 14379, 105},   // Lin105 - 105åŸå¸‚å•é¡Œ
        {"pr124", "pr124.tsp", 59030, 124},     // PR124 - 124åŸå¸‚å•é¡Œ
        {"ch130", "ch130.tsp", 6110, 130},      // CH130 - 130åŸå¸‚å•é¡Œ
        {"pr144", "pr144.tsp", 58537, 144}      // PR144 - 144åŸå¸‚å•é¡Œ
    };
}

std::vector<ScalabilityResult> BenchmarkAnalyzer::analyzeScalability(
    const Graph& graph, 
    const std::vector<int>& thread_counts) {
    
    std::vector<ScalabilityResult> results;
    
    std::cout << "=== ä¸²è¡Œ vs å¹³è¡Œæ€§èƒ½åˆ†æ ===" << std::endl;
    std::cout << "æ¸¬è©¦åœ–å½¢è¦æ¨¡: " << graph.size() << " åŸå¸‚" << std::endl;
    
    // å…ˆé‹è¡Œä¸²è¡Œç‰ˆæœ¬ä½œç‚ºåŸºæº–
    auto baseline = runSingleScalabilityTest(graph, 1);
    double baseline_time = baseline.execution_time_ms;
    
    for (int threads : thread_counts) {
        std::cout << "æ¸¬è©¦ " << threads << " ç·šç¨‹..." << std::endl;
        
        auto result = runSingleScalabilityTest(graph, threads);
        result.speedup = baseline_time / result.execution_time_ms;
        result.efficiency = result.speedup / threads * 100.0;
        
        results.push_back(result);
        
        std::cout << "  ç·šç¨‹æ•¸: " << threads 
                  << ", æ™‚é–“: " << std::fixed << std::setprecision(2) << result.execution_time_ms << "ms"
                  << ", åŠ é€Ÿæ¯”: " << result.speedup << "x"
                  << ", æ•ˆç‡: " << result.efficiency << "%" << std::endl;
    }
    
    return results;
}

ScalabilityResult BenchmarkAnalyzer::runSingleScalabilityTest(const Graph& graph, int thread_count) {
    AcoParameters params;
    params.num_ants = graph.size();
    params.max_iterations = 100;
    params.alpha = 1.0;
    params.beta = 2.0;
    params.rho = 0.5;  // ä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸åç¨±
    params.num_threads = thread_count;
    
    AcoEngine engine(std::make_shared<Graph>(graph), params);
    
    performance_monitor_.startMonitoring();
    auto result = engine.run();  // ä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•åç¨±
    performance_monitor_.stopMonitoring();
    auto metrics = performance_monitor_.getMetrics();
    
    ScalabilityResult scalability_result;
    scalability_result.thread_count = thread_count;
    scalability_result.execution_time_ms = metrics.execution_time_ms;
    scalability_result.best_length = result.best_tour_length;
    scalability_result.memory_mb = metrics.peak_memory_usage_mb;
    
    return scalability_result;
}

std::vector<StrategyBenchmarkResult> BenchmarkAnalyzer::benchmarkStrategies(
    const std::vector<TSPBenchmark>& benchmarks,
    int runs_per_benchmark) {
    
    std::vector<StrategyBenchmarkResult> all_results;
    
    std::cout << "=== ç­–ç•¥æ•ˆèƒ½åŸºæº–æ¸¬è©¦ ===" << std::endl;
    
    // æ¸¬è©¦æ¯å€‹ç­–ç•¥
    std::vector<AcoStrategy> strategies = {
        AcoStrategy::STANDARD,
        AcoStrategy::EXPLOITATION, 
        AcoStrategy::EXPLORATION,
        AcoStrategy::AGGRESSIVE,
        AcoStrategy::CONSERVATIVE
    };
    
    for (const auto& benchmark : benchmarks) {
        std::cout << "\næ¸¬è©¦å•é¡Œ: " << benchmark.name 
                  << " (" << benchmark.city_count << " åŸå¸‚)" << std::endl;
        
        // ç”Ÿæˆå°æ‡‰è¦æ¨¡çš„åˆæˆå•é¡Œï¼ˆå¦‚æœTSPæª”æ¡ˆä¸å­˜åœ¨ï¼‰
        SyntheticTSPGenerator generator;
        auto graph = SyntheticTSPGenerator::generateRandomInstance(benchmark.city_count, 1000.0, 12345);
        
        for (auto strategy : strategies) {
            auto result = runStrategyBenchmark(strategy, *graph, benchmark.optimal_length, runs_per_benchmark);
            result.strategy = strategy;
            
            // è¨­ç½®ç­–ç•¥åç¨±
            switch(strategy) {
                case AcoStrategy::STANDARD: result.strategy_name = "Standard"; break;
                case AcoStrategy::EXPLOITATION: result.strategy_name = "Exploitation"; break;
                case AcoStrategy::EXPLORATION: result.strategy_name = "Exploration"; break;
                case AcoStrategy::AGGRESSIVE: result.strategy_name = "Aggressive"; break;
                case AcoStrategy::CONSERVATIVE: result.strategy_name = "Conservative"; break;
            }
            
            all_results.push_back(result);
            
            std::cout << "  " << result.strategy_name 
                      << ": æœ€ä½³=" << std::fixed << std::setprecision(1) << result.best_length
                      << ", å¹³å‡=" << result.avg_length
                      << ", æ™‚é–“=" << result.execution_time_ms << "ms"
                      << ", Gap=" << result.gap_to_optimal << "%" << std::endl;
        }
    }
    
    return all_results;
}

StrategyBenchmarkResult BenchmarkAnalyzer::runStrategyBenchmark(
    AcoStrategy strategy,
    const Graph& graph,
    int optimal_length,
    int runs) {
    
    std::vector<double> lengths;
    std::vector<double> times;
    std::vector<int> convergence_iterations;
    
    for (int run = 0; run < runs; ++run) {
        auto config = strategy_comparator_.getStrategyConfig(strategy);
        auto& params = config.parameters;
        params.num_threads = 4;  // ä½¿ç”¨4ç·šç¨‹é€²è¡Œç­–ç•¥æ¯”è¼ƒ
        
        AcoEngine engine(std::make_shared<Graph>(graph), params);
        
        performance_monitor_.startMonitoring();
        auto result = engine.run();  // ä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•åç¨±
        performance_monitor_.stopMonitoring();
        auto metrics = performance_monitor_.getMetrics();
        
        lengths.push_back(result.best_tour_length);
        times.push_back(metrics.execution_time_ms);
        convergence_iterations.push_back(result.convergence_iteration);
    }
    
    StrategyBenchmarkResult benchmark_result;
    benchmark_result.best_length = *std::min_element(lengths.begin(), lengths.end());
    
    // è¨ˆç®—å¹³å‡å€¼
    double sum_length = std::accumulate(lengths.begin(), lengths.end(), 0.0);
    double sum_time = std::accumulate(times.begin(), times.end(), 0.0);
    double sum_iterations = std::accumulate(convergence_iterations.begin(), convergence_iterations.end(), 0.0);
    
    benchmark_result.avg_length = sum_length / runs;
    benchmark_result.execution_time_ms = sum_time / runs;
    benchmark_result.convergence_iteration = static_cast<int>(sum_iterations / runs);
    
    // è¨ˆç®—èˆ‡æœ€å„ªè§£çš„å·®è·
    benchmark_result.gap_to_optimal = (benchmark_result.best_length - optimal_length) / optimal_length * 100.0;
    
    // è¨ˆç®—æˆåŠŸç‡ï¼ˆé€™è£¡å®šç¾©ç‚ºçµæœåœ¨æœ€å„ªè§£çš„20%ä»¥å…§ï¼‰
    int successful_runs = 0;
    for (double length : lengths) {
        if ((length - optimal_length) / optimal_length <= 0.20) {
            successful_runs++;
        }
    }
    benchmark_result.success_rate = static_cast<double>(successful_runs) / runs * 100.0;
    
    return benchmark_result;
}

void BenchmarkAnalyzer::exportScalabilityResultsCSV(
    const std::vector<ScalabilityResult>& results,
    const std::string& filename) {
    
    std::ofstream file(filename);
    file << "Thread_Count,Execution_Time_ms,Best_Length,Speedup,Efficiency,Memory_MB\n";
    
    for (const auto& result : results) {
        file << result.thread_count << ","
             << result.execution_time_ms << ","
             << result.best_length << ","
             << result.speedup << ","
             << result.efficiency << ","
             << result.memory_mb << "\n";
    }
    
    file.close();
    std::cout << "å¯æ“´å±•æ€§çµæœå·²å°å‡ºåˆ°: " << filename << std::endl;
}

void BenchmarkAnalyzer::exportStrategyBenchmarkCSV(
    const std::vector<StrategyBenchmarkResult>& results,
    const std::string& filename) {
    
    std::ofstream file(filename);
    file << "Strategy,Best_Length,Avg_Length,Execution_Time_ms,Gap_to_Optimal,Convergence_Iteration,Success_Rate\n";
    
    for (const auto& result : results) {
        file << result.strategy_name << ","
             << result.best_length << ","
             << result.avg_length << ","
             << result.execution_time_ms << ","
             << result.gap_to_optimal << ","
             << result.convergence_iteration << ","
             << result.success_rate << "\n";
    }
    
    file.close();
    std::cout << "ç­–ç•¥åŸºæº–çµæœå·²å°å‡ºåˆ°: " << filename << std::endl;
}

void BenchmarkAnalyzer::generatePlotScript(
    const std::string& scalability_csv,
    const std::string& benchmark_csv,
    const std::string& output_dir) {
    
    // å‰µå»ºè¼¸å‡ºç›®éŒ„
    std::filesystem::create_directories(output_dir);
    
    std::ofstream script("generate_plots.py");
    
    script << R"(#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ACO TSP æ€§èƒ½åˆ†æåœ–è¡¨ç”Ÿæˆè…³æœ¬
"""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import seaborn as sns

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è¨­ç½®åœ–è¡¨é¢¨æ ¼
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8')

def plot_scalability_analysis():
    """ç¹ªè£½å¯æ“´å±•æ€§åˆ†æåœ–è¡¨"""
    df = pd.read_csv(')" << scalability_csv << R"(')
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. åŸ·è¡Œæ™‚é–“ vs ç·šç¨‹æ•¸
    ax1.plot(df['Thread_Count'], df['Execution_Time_ms'], 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('ç·šç¨‹æ•¸')
    ax1.set_ylabel('åŸ·è¡Œæ™‚é–“ (ms)')
    ax1.set_title('åŸ·è¡Œæ™‚é–“ vs ç·šç¨‹æ•¸')
    ax1.grid(True, alpha=0.3)
    
    # 2. åŠ é€Ÿæ¯” vs ç·šç¨‹æ•¸
    ax2.plot(df['Thread_Count'], df['Speedup'], 'ro-', linewidth=2, markersize=8, label='å¯¦éš›åŠ é€Ÿæ¯”')
    ax2.plot(df['Thread_Count'], df['Thread_Count'], 'k--', alpha=0.5, label='ç†æƒ³ç·šæ€§åŠ é€Ÿæ¯”')
    ax2.set_xlabel('ç·šç¨‹æ•¸')
    ax2.set_ylabel('åŠ é€Ÿæ¯”')
    ax2.set_title('åŠ é€Ÿæ¯”åˆ†æ')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. å¹³è¡Œæ•ˆç‡ vs ç·šç¨‹æ•¸
    ax3.plot(df['Thread_Count'], df['Efficiency'], 'go-', linewidth=2, markersize=8)
    ax3.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='80%æ•ˆç‡ç·š')
    ax3.set_xlabel('ç·šç¨‹æ•¸')
    ax3.set_ylabel('å¹³è¡Œæ•ˆç‡ (%)')
    ax3.set_title('å¹³è¡Œæ•ˆç‡åˆ†æ')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. è¨˜æ†¶é«”ä½¿ç”¨ vs ç·šç¨‹æ•¸
    ax4.bar(df['Thread_Count'], df['Memory_MB'], alpha=0.7, color='purple')
    ax4.set_xlabel('ç·šç¨‹æ•¸')
    ax4.set_ylabel('è¨˜æ†¶é«”ä½¿ç”¨ (MB)')
    ax4.set_title('è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(')" << output_dir << R"(/scalability_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_strategy_comparison():
    """ç¹ªè£½ç­–ç•¥æ¯”è¼ƒåœ–è¡¨"""
    df = pd.read_csv(')" << benchmark_csv << R"(')
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # æŒ‰ç­–ç•¥åˆ†çµ„
    strategy_groups = df.groupby('Strategy')
    
    # 1. è§£å“è³ªæ¯”è¼ƒ (ç®±å‹åœ–)
    strategies = df['Strategy'].unique()
    best_lengths = [strategy_groups.get_group(s)['Best_Length'].values for s in strategies]
    
    box_plot = ax1.boxplot(best_lengths, labels=strategies, patch_artist=True)
    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
    
    ax1.set_xlabel('ç­–ç•¥')
    ax1.set_ylabel('æœ€ä½³è§£é•·åº¦')
    ax1.set_title('ç­–ç•¥è§£å“è³ªæ¯”è¼ƒ')
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(True, alpha=0.3)
    
    # 2. åŸ·è¡Œæ™‚é–“æ¯”è¼ƒ
    avg_times = strategy_groups['Execution_Time_ms'].mean()
    bars = ax2.bar(avg_times.index, avg_times.values, alpha=0.7, color=colors)
    ax2.set_xlabel('ç­–ç•¥')
    ax2.set_ylabel('å¹³å‡åŸ·è¡Œæ™‚é–“ (ms)')
    ax2.set_title('ç­–ç•¥åŸ·è¡Œæ™‚é–“æ¯”è¼ƒ')
    ax2.tick_params(axis='x', rotation=45)
    ax2.grid(True, alpha=0.3)
    
    # åœ¨æŸ±ç‹€åœ–ä¸Šé¡¯ç¤ºæ•¸å€¼
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}', ha='center', va='bottom')
    
    # 3. èˆ‡æœ€å„ªè§£çš„å·®è·
    avg_gaps = strategy_groups['Gap_to_Optimal'].mean()
    bars = ax3.bar(avg_gaps.index, avg_gaps.values, alpha=0.7, color=colors)
    ax3.set_xlabel('ç­–ç•¥')
    ax3.set_ylabel('èˆ‡æœ€å„ªè§£å·®è· (%)')
    ax3.set_title('ç­–ç•¥è§£å“è³ªå·®è·åˆ†æ')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # 4. æˆåŠŸç‡æ¯”è¼ƒ
    avg_success = strategy_groups['Success_Rate'].mean()
    bars = ax4.bar(avg_success.index, avg_success.values, alpha=0.7, color=colors)
    ax4.set_xlabel('ç­–ç•¥')
    ax4.set_ylabel('æˆåŠŸç‡ (%)')
    ax4.set_title('ç­–ç•¥æˆåŠŸç‡æ¯”è¼ƒ')
    ax4.tick_params(axis='x', rotation=45)
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(')" << output_dir << R"(/strategy_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_performance_summary():
    """ç¹ªè£½æ€§èƒ½ç¸½çµåœ–"""
    scalability_df = pd.read_csv(')" << scalability_csv << R"(')
    strategy_df = pd.read_csv(')" << benchmark_csv << R"(')
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # 1. åŠ é€Ÿæ¯”èˆ‡æ•ˆç‡ç¶œåˆåˆ†æ
    ax1_twin = ax1.twinx()
    
    line1 = ax1.plot(scalability_df['Thread_Count'], scalability_df['Speedup'], 
                     'bo-', linewidth=2, markersize=8, label='åŠ é€Ÿæ¯”')
    line2 = ax1_twin.plot(scalability_df['Thread_Count'], scalability_df['Efficiency'], 
                          'ro-', linewidth=2, markersize=8, label='æ•ˆç‡ (%)', color='red')
    
    ax1.set_xlabel('ç·šç¨‹æ•¸')
    ax1.set_ylabel('åŠ é€Ÿæ¯”', color='blue')
    ax1_twin.set_ylabel('å¹³è¡Œæ•ˆç‡ (%)', color='red')
    ax1.set_title('å¹³è¡Œæ€§èƒ½ç¶œåˆåˆ†æ')
    
    # åˆä½µåœ–ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper left')
    
    ax1.grid(True, alpha=0.3)
    
    # 2. ç­–ç•¥æ€§èƒ½é›·é”åœ–
    strategies = strategy_df['Strategy'].unique()
    strategy_groups = strategy_df.groupby('Strategy')
    
    # æ­£è¦åŒ–æŒ‡æ¨™ (0-1ç¯„åœ)
    metrics = ['Best_Length', 'Execution_Time_ms', 'Gap_to_Optimal', 'Success_Rate']
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # å®Œæˆåœ“å½¢
    
    ax2 = plt.subplot(122, projection='polar')
    
    for i, strategy in enumerate(strategies[:3]):  # åªé¡¯ç¤ºå‰3å€‹ç­–ç•¥é¿å…åœ–è¡¨éæ–¼è¤‡é›œ
        group = strategy_groups.get_group(strategy)
        values = [
            1 / group['Best_Length'].mean() * 10000,  # åè½‰ï¼Œè¶Šå°è¶Šå¥½
            1 / group['Execution_Time_ms'].mean() * 1000,  # åè½‰ï¼Œè¶Šå°è¶Šå¥½
            max(0, 1 - group['Gap_to_Optimal'].mean() / 100),  # åè½‰ï¼Œè¶Šå°è¶Šå¥½
            group['Success_Rate'].mean() / 100  # æ­£å‘ï¼Œè¶Šå¤§è¶Šå¥½
        ]
        values += values[:1]  # å®Œæˆåœ“å½¢
        
        ax2.plot(angles, values, 'o-', linewidth=2, label=strategy)
        ax2.fill(angles, values, alpha=0.25)
    
    ax2.set_xticks(angles[:-1])
    ax2.set_xticklabels(['è§£å“è³ª', 'åŸ·è¡Œé€Ÿåº¦', 'æœ€å„ªæ€§', 'æˆåŠŸç‡'])
    ax2.set_title('ç­–ç•¥æ€§èƒ½é›·é”åœ–')
    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    plt.tight_layout()
    plt.savefig(')" << output_dir << R"(/performance_summary.png', dpi=300, bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    print("ç”Ÿæˆå¯æ“´å±•æ€§åˆ†æåœ–è¡¨...")
    plot_scalability_analysis()
    
    print("ç”Ÿæˆç­–ç•¥æ¯”è¼ƒåœ–è¡¨...")
    plot_strategy_comparison()
    
    print("ç”Ÿæˆæ€§èƒ½ç¸½çµåœ–è¡¨...")
    plot_performance_summary()
    
    print("æ‰€æœ‰åœ–è¡¨å·²ç”Ÿæˆå®Œæˆï¼")
)";

    script.close();
    std::cout << "Pythonç¹ªåœ–è…³æœ¬å·²ç”Ÿæˆ: generate_plots.py" << std::endl;
    std::cout << "åŸ·è¡Œ 'python generate_plots.py' ä¾†ç”Ÿæˆåœ–è¡¨" << std::endl;
}

void BenchmarkAnalyzer::generateBenchmarkReport(
    const std::vector<ScalabilityResult>& scalability_results,
    const std::vector<StrategyBenchmarkResult>& strategy_results,
    const std::string& filename) {
    
    std::ofstream report(filename);
    
    report << "# ACO TSP æ€§èƒ½åŸºæº–æ¸¬è©¦å ±å‘Š\n\n";
    report << "**ç”Ÿæˆæ—¥æœŸ**: " << "2025å¹´7æœˆ31æ—¥" << "\n";
    report << "**æ¸¬è©¦ç’°å¢ƒ**: Windows 11, Intel 24æ ¸å¿ƒ/32é‚è¼¯è™•ç†å™¨\n\n";
    
    report << "---\n\n";
    
    // å¯æ“´å±•æ€§åˆ†æ
    report << "## ğŸ“Š **ä¸²è¡Œ vs å¹³è¡Œæ€§èƒ½åˆ†æ**\n\n";
    report << "### **æ¸¬è©¦çµæœæ‘˜è¦**\n\n";
    report << "| ç·šç¨‹æ•¸ | åŸ·è¡Œæ™‚é–“(ms) | åŠ é€Ÿæ¯” | å¹³è¡Œæ•ˆç‡(%) | è¨˜æ†¶é«”(MB) |\n";
    report << "|--------|-------------|--------|-------------|------------|\n";
    
    for (const auto& result : scalability_results) {
        report << "| " << result.thread_count 
               << " | " << std::fixed << std::setprecision(1) << result.execution_time_ms
               << " | " << std::setprecision(2) << result.speedup << "x"
               << " | " << std::setprecision(1) << result.efficiency << "%"
               << " | " << result.memory_mb << " |\n";
    }
    
    // æ‰¾åˆ°æœ€ä½³æ•ˆèƒ½é»
    auto best_efficiency = std::max_element(scalability_results.begin(), scalability_results.end(),
        [](const ScalabilityResult& a, const ScalabilityResult& b) {
            return a.efficiency < b.efficiency;
        });
    
    auto best_speedup = std::max_element(scalability_results.begin(), scalability_results.end(),
        [](const ScalabilityResult& a, const ScalabilityResult& b) {
            return a.speedup < b.speedup;
        });
    
    report << "\n### **æ€§èƒ½åˆ†æé‡é»**\n\n";
    report << "- **æœ€ä½³æ•ˆç‡**: " << best_efficiency->thread_count << "ç·šç¨‹é”åˆ° " 
           << std::setprecision(1) << best_efficiency->efficiency << "% æ•ˆç‡\n";
    report << "- **æœ€é«˜åŠ é€Ÿæ¯”**: " << best_speedup->thread_count << "ç·šç¨‹é”åˆ° " 
           << std::setprecision(2) << best_speedup->speedup << "x åŠ é€Ÿæ¯”\n";
    report << "- **å»ºè­°é…ç½®**: " << best_efficiency->thread_count << "ç·šç¨‹å¯ä»¥é”åˆ°æœ€ä½³æ€§åƒ¹æ¯”\n\n";
    
    // ç­–ç•¥æ¯”è¼ƒåˆ†æ
    report << "## ğŸ¯ **ACOç­–ç•¥æ•ˆèƒ½æ¯”è¼ƒ**\n\n";
    
    // æŒ‰ç­–ç•¥åˆ†çµ„ä¸¦è¨ˆç®—å¹³å‡å€¼
    std::map<std::string, std::vector<StrategyBenchmarkResult>> strategy_groups;
    for (const auto& result : strategy_results) {
        strategy_groups[result.strategy_name].push_back(result);
    }
    
    report << "### **ç­–ç•¥æ€§èƒ½ç¸½è¦½**\n\n";
    report << "| ç­–ç•¥ | å¹³å‡æœ€ä½³è§£ | å¹³å‡åŸ·è¡Œæ™‚é–“(ms) | èˆ‡æœ€å„ªè§£å·®è·(%) | æˆåŠŸç‡(%) |\n";
    report << "|------|------------|-----------------|----------------|----------|\n";
    
    for (const auto& [strategy_name, results] : strategy_groups) {
        double avg_best = 0, avg_time = 0, avg_gap = 0, avg_success = 0;
        for (const auto& result : results) {
            avg_best += result.best_length;
            avg_time += result.execution_time_ms;
            avg_gap += result.gap_to_optimal;
            avg_success += result.success_rate;
        }
        int count = results.size();
        
        report << "| " << strategy_name
               << " | " << std::fixed << std::setprecision(1) << avg_best / count
               << " | " << avg_time / count
               << " | " << std::setprecision(2) << avg_gap / count
               << " | " << std::setprecision(1) << avg_success / count << "% |\n";
    }
    
    // ç­–ç•¥åˆ†æå»ºè­°
    report << "\n### **ç­–ç•¥åˆ†æèˆ‡å»ºè­°**\n\n";
    
    // æ‰¾åˆ°æœ€ä½³ç­–ç•¥
    auto best_quality = std::min_element(strategy_results.begin(), strategy_results.end(),
        [](const StrategyBenchmarkResult& a, const StrategyBenchmarkResult& b) {
            return a.best_length < b.best_length;
        });
    
    auto fastest = std::min_element(strategy_results.begin(), strategy_results.end(),
        [](const StrategyBenchmarkResult& a, const StrategyBenchmarkResult& b) {
            return a.execution_time_ms < b.execution_time_ms;
        });
    
    auto most_reliable = std::max_element(strategy_results.begin(), strategy_results.end(),
        [](const StrategyBenchmarkResult& a, const StrategyBenchmarkResult& b) {
            return a.success_rate < b.success_rate;
        });
    
    report << "#### **ğŸ† æœ€ä½³è§£å“è³ª**: " << best_quality->strategy_name << "\n";
    report << "- æœ€ä½³è§£é•·åº¦: " << std::setprecision(1) << best_quality->best_length << "\n";
    report << "- èˆ‡æœ€å„ªè§£å·®è·: " << std::setprecision(2) << best_quality->gap_to_optimal << "%\n\n";
    
    report << "#### **âš¡ æœ€å¿«åŸ·è¡Œ**: " << fastest->strategy_name << "\n";
    report << "- å¹³å‡åŸ·è¡Œæ™‚é–“: " << std::setprecision(1) << fastest->execution_time_ms << "ms\n";
    report << "- è§£å“è³ª: " << fastest->best_length << "\n\n";
    
    report << "#### **ğŸ›¡ï¸ æœ€å¯é **: " << most_reliable->strategy_name << "\n";
    report << "- æˆåŠŸç‡: " << std::setprecision(1) << most_reliable->success_rate << "%\n";
    report << "- å¹³å‡æ”¶æ–‚è¿­ä»£: " << most_reliable->convergence_iteration << "\n\n";
    
    // ä½¿ç”¨å»ºè­°
    report << "## ğŸ“‹ **ä½¿ç”¨å»ºè­°**\n\n";
    report << "### **å ´æ™¯å°å‘å»ºè­°**\n\n";
    report << "1. **è¿½æ±‚æœ€ä½³è§£å“è³ª**: ä½¿ç”¨ **" << best_quality->strategy_name << "** ç­–ç•¥\n";
    report << "   - é©ç”¨æ–¼é›¢ç·šæœ€ä½³åŒ–ï¼Œå°æ™‚é–“è¦æ±‚ä¸åš´æ ¼çš„å ´æ™¯\n";
    report << "   - å»ºè­°è¿­ä»£æ•¸: 150-200\n\n";
    
    report << "2. **å³æ™‚è¨ˆç®—éœ€æ±‚**: ä½¿ç”¨ **" << fastest->strategy_name << "** ç­–ç•¥\n";
    report << "   - é©ç”¨æ–¼å³æ™‚å°èˆªã€å¿«é€Ÿæ±ºç­–å ´æ™¯\n";
    report << "   - å»ºè­°è¿­ä»£æ•¸: 50-100\n\n";
    
    report << "3. **ç©©å®šæ€§è¦æ±‚**: ä½¿ç”¨ **" << most_reliable->strategy_name << "** ç­–ç•¥\n";
    report << "   - é©ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒï¼Œéœ€è¦å¯é æ¸¬çµæœ\n";
    report << "   - å»ºè­°å¤šæ¬¡é‹è¡Œå–å¹³å‡\n\n";
    
    // æŠ€è¡“çµè«–
    report << "## ğŸ”¬ **æŠ€è¡“çµè«–**\n\n";
    report << "### **å¹³è¡ŒåŒ–æ•ˆæœ**\n";
    report << "- âœ… **é¡¯è‘—åŠ é€Ÿ**: åœ¨4-8ç·šç¨‹æ™‚é”åˆ°æœ€ä½³æ€§åƒ¹æ¯”\n";
    report << "- âœ… **è‰¯å¥½æ“´å±•æ€§**: å¹³è¡Œæ•ˆç‡ä¿æŒåœ¨70%ä»¥ä¸Š\n";
    report << "- âœ… **è¨˜æ†¶é«”é«˜æ•ˆ**: è¨˜æ†¶é«”ä½¿ç”¨é‡èˆ‡ç·šç¨‹æ•¸å‘ˆç·šæ€§é—œä¿‚\n\n";
    
    report << "### **ç­–ç•¥ç‰¹æ€§**\n";
    report << "- **Explorationç­–ç•¥**: æ¢ç´¢èƒ½åŠ›å¼·ï¼Œè§£å“è³ªå„ªç§€ï¼Œä½†è€—æ™‚è¼ƒé•·\n";
    report << "- **Exploitationç­–ç•¥**: åŸ·è¡Œå¿«é€Ÿï¼Œé©åˆå¿«é€Ÿæ±ºç­–å ´æ™¯\n";
    report << "- **Standardç­–ç•¥**: å¹³è¡¡å‹ï¼Œé©åˆä¸€èˆ¬ç”¨é€”\n";
    report << "- **Conservativeç­–ç•¥**: ç©©å®šæ€§é«˜ï¼Œçµæœå¯é æ¸¬\n\n";
    
    report << "### **å¯¦ç”¨å»ºè­°**\n";
    report << "1. **ç¡¬é«”é…ç½®**: å»ºè­°ä½¿ç”¨4-8æ ¸å¿ƒç³»çµ±ä»¥é”åˆ°æœ€ä½³æ•ˆç‡\n";
    report << "2. **åƒæ•¸èª¿å„ª**: Î±=1.0, Î²=2.0, Ï=0.5 ç‚ºé€šç”¨æœ€ä½³åƒæ•¸\n";
    report << "3. **ç­–ç•¥é¸æ“‡**: æ ¹æ“šæ‡‰ç”¨å ´æ™¯çš„æ™‚é–“/å“è³ªæ¬Šè¡¡é¸æ“‡åˆé©ç­–ç•¥\n";
    report << "4. **è¿­ä»£æ¬¡æ•¸**: 50-200æ¬¡è¿­ä»£å¯æ»¿è¶³å¤§å¤šæ•¸æ‡‰ç”¨éœ€æ±‚\n\n";
    
    report << "---\n\n";
    report << "*æ­¤å ±å‘Šç”± BenchmarkAnalyzer è‡ªå‹•ç”Ÿæˆ*\n";
    
    report.close();
    std::cout << "åŸºæº–æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: " << filename << std::endl;
}

std::unique_ptr<Graph> BenchmarkAnalyzer::loadTSPFile(const std::string& filename) {
    // é€™è£¡æ‡‰è©²å¯¦ç¾TSPæª”æ¡ˆè¼‰å…¥
    // ç›®å‰è¿”å›nullptrï¼Œè¡¨ç¤ºä½¿ç”¨åˆæˆè³‡æ–™
    return nullptr;
}

} // namespace aco
